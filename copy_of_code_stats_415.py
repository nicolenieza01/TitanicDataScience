# -*- coding: utf-8 -*-
"""Copy of CODE STATS 415

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZFy4lQ7w96YWQ7VabHMgRXjXGUlSa90C
"""

from google.colab import files
uploaded = files.upload()

pip install sqldf

# Commented out IPython magic to ensure Python compatibility.
#importing general libraries
import numpy as np;
from scipy import stats;
from scipy.stats import mode;
import pandas as pd;
import sqldf;
#from pandasql import sqldf;
import regex as re;

import matplotlib.pyplot as plt;
# %matplotlib inline
import seaborn as sns;
sns.set_style('darkgrid');

from warnings import filterwarnings;
from termcolor import colored;

from tqdm.notebook import tqdm;

np.random.seed(10);

#importing model specific libraries
from sklearn_pandas import DataFrameMapper, gen_features;

from sklearn.compose import make_column_selector;
from sklearn.base import BaseEstimator, TransformerMixin;
from sklearn.pipeline import make_pipeline, Pipeline ;
from sklearn.preprocessing import FunctionTransformer, LabelEncoder, StandardScaler, RobustScaler, OrdinalEncoder;
from sklearn.impute import SimpleImputer;

from sklearn.model_selection import KFold, GridSearchCV;
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier;
from xgboost import XGBClassifier;
from lightgbm import LGBMClassifier;
from sklearn.svm import SVC;
from sklearn.tree import DecisionTreeClassifier;
from sklearn.linear_model import LogisticRegression;

from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, f1_score, classification_report, confusion_matrix;

import io
xytrain = pd.read_csv(io.BytesIO(uploaded['train.csv']))
xtest = pd.read_csv(io.BytesIO(uploaded['test.csv']))
# Dataset is now stored in a Pandas Dataframe

xtrain, ytrain = xytrain.drop('Survived', axis= 1), xytrain[['Survived']];

print(colored(F"Train-Test dataframe lengths = {len(xytrain), len(xtest)}"));
print(colored(F"\nTrain-set information\n"))
display(xytrain.info());
print(colored(F"\nTrain-set description\n"));
display(xytrain.describe().style.format('{:.2f}'));

#data processsing and visualization
plt.subplots(1,1,figsize= (6,6));
ax = ytrain.value_counts().plot.bar(color= 'tab:red');
ax.set_title("Surviver analysis for the training data set", color = 'tab:pink');
ax.set_xlabel('Survival status', color= 'tab:pink');
ax.set_yticks(range(0, len(xtrain), 50));
ax.set_ylabel('Passengers', color= 'tab:pink');
plt.xticks(rotation = 0);
plt.show();

_ = xytrain.groupby(['Sex', 'Pclass']).agg(Survivors = pd.NamedAgg('Survived', np.sum),Passengers = pd.NamedAgg('Survived', np.size)).sort_index(level=[1,0])
_['Survival_Rate'] = _['Survivors'] / _['Passengers'];
print(colored(f'\nSurvival rate by gender and pclass\n', color = 'pink', attrs= ['bold', 'dark']));
display(_.style.format({'Survival_Rate':'{:.2%}'}))

_0 = _.groupby(level= 0).agg({'Survivors':np.sum, 'Passengers':np.sum});
_0['Survival_Rate'] = _0['Survivors'] / _0['Passengers'];

_1 = _.groupby(level= 1).agg({'Survivors':np.sum, 'Passengers':np.sum});
_1['Survival_Rate'] = _1['Survivors'] / _1['Passengers'];

print('\n');
fig, ax = plt.subplots(nrows= 1,ncols=2,figsize = (12,6), sharey= True);
sns.barplot(x = _0.index, y = _0.Survival_Rate, palette = 'Reds', ax = ax[0]);
sns.barplot(x = _1.index, y = _1.Survival_Rate, palette = 'Reds', ax= ax[1]);
ax[0].set_title("Survival analysis by gender", color = 'tab:pink', fontsize = 12);
ax[1].set_title("Survival analysis by passenger class", color = 'tab:pink', fontsize = 12);
plt.yticks(np.arange(0,1,0.05),fontsize= 8, color = 'pink');
plt.show()

del _, _0, _1;

fig, ax = plt.subplots(1,2,figsize = (18,6));
sns.histplot(x = xytrain['Age'], kde= True, palette = 'Reds', ax = ax[0]);
ax[0].set_title(f"Overall age distribution analysis", color = 'tab:red' )

sns.boxplot(x = xytrain.Pclass, y = xytrain.Age, palette = 'Reds', ax = ax[1]);
ax[1].set_title(f"Age distribution per Pclass", color = 'tab:red');

plt.show();

print(colored(f"\nTicket fare by Pclass and survivorship\n", color = 'blue', attrs= ['bold', 'dark']));
display(xytrain.groupby(['Pclass', 'Survived']).agg({'Fare': [np.amin, np.median, np.mean, np.amax]}).style.format('{:.2f}'))

xytrain.head()

xytrain.shape

xytrain.dtypes

"""Creating a family variable and deleting unneccesary rows"""

xytrain = pd.read_csv(io.BytesIO(uploaded['train.csv']))
xytrain['Family'] = xytrain['SibSp'] + xytrain['Parch'] + 1

xytrain = xytrain.drop('Name', axis = 1,)
xytrain = xytrain.drop('Ticket', axis = 1,)
xytrain = xytrain.drop('Fare', axis = 1,)
xytrain = xytrain.drop('Cabin', axis = 1,)

xytrain = xytrain.drop('SibSp', axis = 1,)
xytrain = xytrain.drop('Parch', axis = 1,)

xytrain.describe()

xytrain['Embarked'].value_counts()

#finding number of NA's in Embarked column
xytrain['Embarked'].isna().sum()

# LOGISTIC REGRESSION ********WORKS WITH 81.00% ACCURACY********
# KNN *********WORKS WITH 63.68%************
# RANDOM FORESTS ********WORKS WITH 84.4%*****************


# DATA CLEANSING AND SOME MORE VISUALIZATION
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from google.colab import files
import pandas as pd;
uploaded = files.upload()
import io
train = pd.read_csv(io.BytesIO(uploaded['train.csv']))
test = pd.read_csv(io.BytesIO(uploaded['test.csv']))

# REPLACING MISSING VALUES IN AGE CATEGORY
def add_age(cols):
    Age = cols[0]
    Pclass = cols[1]
    if pd.isnull(Age):
        return int(train[train["Pclass"] == Pclass]["Age"].mean())
    else:
        return Age
train['Age'] = train[['Age','Pclass']].apply(add_age,axis=1)
test['Age'] = test[['Age','Pclass']].apply(add_age,axis=1)

#Missing values beign accounted for and dropping variables with too many nulls
train.drop("Cabin",inplace=True,axis=1)
test.drop("Cabin",inplace=True,axis=1)
train['Embarked'].fillna(train['Embarked'].mode()[0],inplace=True)
test['Embarked'].fillna(test['Embarked'].mode()[0],inplace=True)
test['Fare'].fillna(test['Fare'].mean(),inplace=True)

#MAKING AGE CATEGORICAL BY RANGES AND COMBINING TO MAKE THE FAMILY COLUMN
def combine(df,col1,col2):
    df["Family"] = df[col1]+df[col2]
    df.drop([col1,col2],inplace=True,axis=1)
    return df

train = combine(train,'SibSp','Parch')
test = combine(test,'SibSp','Parch')

def process_age(df,cut_points,label_names):
    df["Age"] = df["Age"].fillna(-0.5)
    df["Age_categories"] = pd.cut(df["Age"],cut_points,labels=label_names)
    return df

cut_points = [-1,0,5,12,18,35,60,100]
label_names = ["Missing","Infant","Child","Teenager","Young Adult","Adult","Senior"]
train = process_age(train,cut_points,label_names)
test = process_age(test,cut_points,label_names)

pivot = train.pivot_table(index="Age_categories",values='Survived')

#ENCODING CATEGORICAL VARIABLES
def create_dummies(df,column_name):
    dummies = pd.get_dummies(df[column_name],prefix=column_name)
    df = pd.concat([df,dummies],axis=1)
    return df

for column in ["Pclass","Sex","Age_categories",'Embarked']:
    train = create_dummies(train,column)
    test = create_dummies(test,column)

#rechecking all NaNs
feat_list = list(train.columns.values)

for feat in feat_list:
  print(feat, ": ", sum(pd.isnull(train[feat])))

#THIS IS GOOD BECAUSE IT SHOWS WE HAVE TAKEN CARE OF ALL NA VALUES

#more exploratory data analyses
def make_pivot (param1, param2):
  df_slice = train[[param1,param2,'PassengerId']]
  slice_pivot = df_slice.pivot_table(index = [param1], columns = [param2],
                                     aggfunc= np.size, fill_value = 0)
  p_chart = slice_pivot.plot.bar()
  for p in p_chart.patches:
    p_chart.annotate(str(p.get_height()),(p.get_x()* 1.05, p.get_height() * 1.01))

  return slice_pivot
  return p_chart

#shows relationship between Survival and Passenger Class
make_pivot("Survived", "Pclass")

#shows relationship between survival and passenger sex
make_pivot("Survived", "Sex")

#shows relationship between survival and port of embarkation
make_pivot("Survived","Embarked")

#shows relationship between survival and family
make_pivot("Survived","Family")

# DECIDING WHICH VARIABLE SHOULD BE DROPPED BASED OFF P VALUES WHEN DATA IS ALREADY CLEANSED

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.feature_selection import chi2, f_regression
import matplotlib.pyplot as plt;
df1 = train.filter(['PassengerId', 'Age', 'Fare','Family',
       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',
       'Age_categories_Missing', 'Age_categories_Infant',
       'Age_categories_Child', 'Age_categories_Teenager',
       'Age_categories_Young Adult', 'Age_categories_Adult',
       'Age_categories_Senior'], axis =1 )
X = df1
df2 = train['Survived']
y= df2

test = SelectKBest(f_classif, k= 'all')
test_fit = test.fit(X,y)
feat_score = test_fit.scores_.round(3)
p_values = -np.log10(test_fit.pvalues_).round(3)

feature_list = list(X.columns.values)
selected_features = test.get_support([test_fit])
selected_features

temporary_lst = []
for i in selected_features:
  temporary_lst.append({'feature': feature_list[i], 'P-value': p_values[i],
                        'score': feat_score[i]})
feat_select = pd.DataFrame(temporary_lst)

feat_select = feat_select.sort_values(by = 'score', axis =0, ascending = False,
                                      inplace = False, kind = 'quicksort',
                                      na_position = 'last')

feat_select = feat_select.set_index('feature')
feat_select

ax = feat_select[['P-value', 'score']].plot(kind = 'bar', title = "P values and scores", legend = True, fontsize = 12)
plt.show()

#WE ARE MAKING OUR DECISIONS ON WHICH VARIABLES TO DROP BASED ON THE RESULTS ABOVE

train.drop(['Name','Sex','Ticket','Pclass','Age_categories','Embarked'],inplace=True,axis=1)
test.drop(['Name','Sex','Ticket','Pclass','Age_categories','Embarked'],inplace=True,axis=1)

#NOW WE ARE ACTUALLY RUNNING THE LOGISITC REGRESSION
lr = LogisticRegression()
columns = ['PassengerId', 'Age', 'Fare','Family',
       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',
       'Age_categories_Missing', 'Age_categories_Infant',
       'Age_categories_Child', 'Age_categories_Teenager',
       'Age_categories_Young Adult', 'Age_categories_Adult',
       'Age_categories_Senior']

lr.fit(train[columns], train["Survived"])

X = train[columns]
y = train['Survived']

train_X, val_X, train_y, val_y = train_test_split(
    X, y, test_size=0.20,random_state=0)



lr = LogisticRegression()
lr.fit(train_X, train_y)
predictions = lr.predict(val_X)
accuracy = accuracy_score(val_y, predictions)
print(accuracy)
from sklearn.metrics import classification_report
print(classification_report(val_y,predictions))

#ACCURACY FOR LOGISTIC REGRESSION IS 81%

#RANDOM FOREST USING SAME FEATURES AS ABOVE
from sklearn.ensemble import RandomForestClassifier
RF_Model = RandomForestClassifier()
RF_Model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=7, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0,
                       min_samples_leaf=1, min_samples_split=6,
                       min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,
                       oob_score=False, random_state=None, verbose=0,
                       warm_start=False)
RF_Model.fit(train_X, train_y)
predictions = RF_Model.predict(val_X)
print(f'Test : {RF_Model.score(val_X, val_y):.3f}')
print(f'Train : {RF_Model.score(train_X, train_y):.3f}')

#ACCURACY FOR RANDOM FORESTS IS 84.4%

#KNN USING SAME FEATURES AS ABOVE
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(train_X, train_y)
y_pred = knn.predict(val_X)
from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(val_y, y_pred))

#ACCURACY FOR KNN IS 63.68%